{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data at Scale\n",
    "\n",
    "By Dr. Phil Winder of https://WinderResearch.com\n",
    "\n",
    "Hi there! Welcome to this notebook which was originally written to be used with the training of the same name. If you like this then please visit my website for more, [tweet @DrPhilWinder](https://twitter.com/DrPhilWinder) or get in touch via [Linkedin](https://www.linkedin.com/in/DrPhilWinder/) or plain old [email](mailto:phil@WinderResearch.com).\n",
    "\n",
    "Table of Contents:\n",
    "\n",
    "1. [Visualising Data](#1.-Visualising-Data)\n",
    "2. [Fixing Missing Data](#2.-Fixing-Missing-Data)\n",
    "3. [Detect and Remove Outliers](#3.-Detect-and-Remove-Outliers)\n",
    "4. [Is it Normal?](#4.-Is-it-Normal?)\n",
    "5. [Fixing Non-Normal Data](#5.-Fixing-Non-Normal-Data)\n",
    "6. [Fixing Scales and Categorical Data](#6:-Fixing-Scales-and-Categorical-Data)\n",
    "7. [Model Improvement through Feature Selection](#7:-Model-Improvement-through-Feature-Selection)\n",
    "8. [Cleaning Timeseries Data](#8:-Cleaning-Timeseries-Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "matplotlib.rc('figure', figsize=[12, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Visualising Data\n",
    "\n",
    "This first section is all about visualising your data. In my opinion, manually visualising data is the most important Data Science technique, but also the most underrepresented.\n",
    "\n",
    "Below I concentrate on demonstrating some techniques that will help you pre-process and clean your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install missingno > /dev/null\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"../data/titanic.csv\"\n",
    "titanic = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.hist(color='dimgray', layout=(2, 4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(titanic.sample(500));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(titanic.sample(500));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.heatmap(titanic);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(titanic, color='dimgray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = titanic.corr()\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=0.8, vmin=-0.8, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"../data/nypd-motor-vehicle-collisions.csv\"\n",
    "collisions = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Challenge\n",
    "\n",
    "Above is the NYPD collisions dataset that reports the causes of traffic incidents in New York (not to be confused with York) in the USA.\n",
    "\n",
    "- Try plotting some visualisations of this data\n",
    "- What can you tell me about it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fixing Missing Data\n",
    "\n",
    "Most real world datasets have missing values. Many examples online and in the literature present data that has already been cleaned. But in industry, significant amounts of data are missing. Let's get some experience working with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"../data/titanic.csv\"\n",
    "titanic = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = titanic[['survived', 'fare', 'age']]\n",
    "titanic.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has a very handy function called `dropna()` that removes all rows that have at least one `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.dropna().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also replace missing values with another value. Zero, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.fillna(0).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or with some function of the columns, like the mean or the median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.fillna(titanic.mean()).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the `age` feature changes quite dramatically depending on what you impute it with. You will probably want to plot the histogram of this in real life to make sure you're not trashing the statistics.\n",
    "\n",
    "We could go further and even create a regression model to predict the ages..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_dropped = titanic.dropna()\n",
    "X = titanic_dropped[['survived', 'fare']]\n",
    "y = titanic_dropped['age']\n",
    "mdl = sklearn.linear_model.LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_X = titanic.dropna(subset = ['survived', 'fare']) # Inputs to an sklearn model can't have NaNs\n",
    "print(titanic_X.count())\n",
    "titanic_imputed = titanic_X.copy()\n",
    "titanic_imputed['age_imputed'] = mdl.predict(titanic_X[['survived', 'fare']])\n",
    "print(titanic_imputed.tail(10))\n",
    "print(titanic_imputed.count())\n",
    "titanic_fixed = titanic_X.copy()\n",
    "titanic_fixed['age'] = titanic_fixed['age'].fillna(titanic_imputed['age_imputed'])\n",
    "print(titanic_fixed.tail(10))\n",
    "print(titanic_fixed.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware, every time we impute the data we're adding subtle biases. For example, if we imputed one feature that was mostly `NaN`s the effect would be that this feature correlates with other features that produced it. If we were performing a classification task, this would be pointless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"../data/titanic.csv\"\n",
    "titanic = pd.read_csv(url)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"../data/property_data.csv\"\n",
    "housing = pd.read_csv(url)\n",
    "housing = housing[['NUM_BATH', 'SQ_FT', 'OWN_OCCUPIED']]\n",
    "housing = housing.replace(['--', 'HURLEY', 'na', '12'], np.nan)\n",
    "housing = housing.replace(['Y'], 1)\n",
    "housing = housing.replace(['N'], 0)\n",
    "housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Challenge\n",
    "\n",
    "There is a very simple hacky dataset above.\n",
    "\n",
    "1. Try dropping `NaN` rows?\n",
    "2. What about replacing with zeros?\n",
    "3. Try to impute using a regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Detect and Remove Outliers\n",
    "\n",
    "Outliers are tricky because we often don't have any ground truth to prove that outliers truly are outliers. So largely this is part art.\n",
    "\n",
    "Here's an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arr = pd.DataFrame([10, 386, 479, 627, 20, 523, 482, 483, 542, 699, 535, 617, 577, 471, 615, 583, 441, 562, 563, 527, 453, 530, 433, 541, 585, 704, 443, 569, 430, 637, 331, 511, 552, 496, 484, 566, 554, 472, 335, 440, 579, 341, 545, 615, 548, 604, 439, 556, 442, 461, 624, 611, 444, 578, 405, 487, 490, 496, 398, 512, 422, 455, 449, 432, 607, 679, 434, 597, 639, 565, 415, 486, 668, 414, 665, 763, 557, 304, 404, 454, 689, 610, 483, 441, 657, 590, 492, 476, 437, 483, 529, 363, 711, 543])\n",
    "arr = (arr - arr.mean()) / arr.std()\n",
    "arr.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = arr[np.abs(arr) < 3]\n",
    "arr.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"../data/winequality-red.csv\"\n",
    "wine = pd.read_csv(url)\n",
    "wine = pd.read_csv(url, sep=\";\")\n",
    "wine = wine[['alcohol', 'quality']]\n",
    "wine.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Challenge\n",
    "\n",
    "Above is a more realistic wine dataset. Can you remove the outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Is it Normal?\n",
    "\n",
    "Normality is important for a range of algorithms. Here we investigate, what is normal? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"../data/starsCYG.csv\"\n",
    "starsCYG = pd.read_csv(url)\n",
    "var = 'log.Te'\n",
    "\n",
    "ax = sns.distplot(starsCYG[var], kde=False, norm_hist=False)\n",
    "ax.set(title='Star Cluster CYG OB1', xlabel='log(Temperature)', ylabel='Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.probplot(\n",
    "    starsCYG[var], plot=sns.mpl.pyplot)\n",
    "ax = plt.gca()\n",
    "ax.get_lines()[0].set_markerfacecolor(sns.color_palette()[0])\n",
    "ax.get_lines()[0].set_markeredgewidth(0)\n",
    "ax.get_lines()[1].set_color(sns.color_palette()[1])\n",
    "ax.set(title='Normal Quantile-Quantile Plot',\n",
    "       xlabel='Theoretical Normal Quantiles', ylabel='Ordered Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution fitting\n",
    "\n",
    "y = starsCYG[var]\n",
    "x_plot = scipy.linspace(min(y) - 1, max(y) + 1, 100)\n",
    "y_count, x = np.histogram(y, density=True)\n",
    "x = (x + np.roll(x, -1))[:-1] / 2.0\n",
    "h = sns.distplot(y, kde=False, norm_hist=True)\n",
    "\n",
    "dist_names = ['gamma', 'beta', 'rayleigh', 'norm', 'pareto', 'powernorm']\n",
    "\n",
    "for dist_name in dist_names:\n",
    "    dist = getattr(scipy.stats, dist_name)\n",
    "    params = dist.fit(y)\n",
    "    # Separate parts of parameters\n",
    "    arg = params[:-2]\n",
    "    loc = params[-2]\n",
    "    scale = params[-1]\n",
    "    pdf_fitted = dist.pdf(x, loc=loc, scale=scale, *arg)\n",
    "    sse = np.sum(np.power(y_count - pdf_fitted, 2.0))\n",
    "    pdf_plot = dist.pdf(x_plot, loc=loc, scale=scale, *arg)\n",
    "    sns.lineplot(x_plot, pdf_plot, label='{} ({:.1f})'.format(dist_name, sse))\n",
    "plt.legend(loc='upper right')\n",
    "ax = plt.gca()\n",
    "ax.set(title='Fitted Distributions (with SSE scores)',\n",
    "       xlabel='log(Temperature)', ylabel='Normalised Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(2 * np.random.randn(200) + 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Challenge\n",
    "\n",
    "Using the data `data`, is it normally distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Fixing Non-Normal Data\n",
    "\n",
    "Once you know that you have some data that is not normal, you can attempt to bring it back to normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"../data/titanic.csv\"\n",
    "titanic = pd.read_csv(url)\n",
    "fare = titanic['fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Challenge\n",
    "\n",
    "Try applying a few mathematical functions to invert this data. Have a go using `np.log`, `np.power`, `np.sqrt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Fixing Scales and Categorical Data\n",
    "\n",
    "Fixing the scales of data is important to make it as easy as possible for the model. This is a classification example - where I limit the number of training iterations to make the point - that breaks because the scales are so skewed. After min-max scaling the data, the model finds it much easier to iterate towards the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "url = \"../data/titanic.csv\"\n",
    "titanic = pd.read_csv(url)\n",
    "titanic = titanic[['survived', 'fare', 'age']]\n",
    "titanic.dropna(inplace=True)\n",
    "titanic = titanic[(titanic[['fare', 'age']] != 0).all(axis=1)]  # Remove zero fares\n",
    "y = titanic['survived']\n",
    "X = titanic[['fare', 'age']]\n",
    "\n",
    "clf = sklearn.svm.LinearSVC(tol=1e-2, max_iter=10).fit(X, y)\n",
    "xx, yy = np.mgrid[0:600:1, 0:100:1]\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "probs = clf.predict(grid).reshape(xx.shape)\n",
    "score = clf.score(X, y) * 100\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "\n",
    "sns.scatterplot(data=X, x='fare', y='age', hue=y, linewidth=0, ax=ax1).set_title(\n",
    "    \"Titanic Survivors - SVM (10 iter) - {:.1f}%\".format(score))\n",
    "ax1.contour(xx, yy, probs, levels=[.5], cmap=\"Greys\", vmin=0, vmax=.6)\n",
    "\n",
    "X[:] = sklearn.preprocessing.MinMaxScaler().fit_transform(X[:])\n",
    "clf = sklearn.svm.LinearSVC(tol=1e-2, max_iter=10).fit(X, y)\n",
    "xx, yy = np.mgrid[0:1:0.001, 0:1:0.001]\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "probs = clf.predict(grid).reshape(xx.shape)\n",
    "score = clf.score(X, y) * 100\n",
    "\n",
    "sns.scatterplot(data=X, x='fare', y='age', hue=y, linewidth=0, ax=ax2).set_title(\n",
    "    \"Titanic Survivors - SVM (10 iter, scaled) - {:.1f}%\".format(score))\n",
    "ax2.contour(xx, yy, probs, levels=[.5], cmap=\"Greys\", vmin=0, vmax=.6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(list('abcaba'))\n",
    "pd.get_dummies(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we're rencoding categorical variables into new features. Now we can pass this data into our standard models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"../data/titanic.csv\"\n",
    "titanic = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic[['age', 'embarked', 'fare', 'survived']]\n",
    "X = X.dropna()\n",
    "y = X[['survived']]\n",
    "X = X[['age', 'embarked', 'fare']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Challenge\n",
    "\n",
    "Above we see the Titanic data again. \n",
    "\n",
    "- Try to recode the `embarked` feature\n",
    "- Then use the features X to predict the label y in a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7: Model Improvement through Feature Selection\n",
    "\n",
    "The Decision Tree classifier (and variants of) attempts to segment the data into \"pure\" buckets via simple thresholds. The split that produces the most \"pure\" bucket is deemed to be a good split.\n",
    "\n",
    "We can use this definition of \"good\" to provide some information about how well a single feature is able to split segment the data according to the labels. \"Better\" features will have a higher score.\n",
    "\n",
    "The example below uses the titanic dataset again to demonstrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.tree\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"../data/titanic.csv\"\n",
    "titanic = pd.read_csv(url)\n",
    "X = titanic[['age', 'fare', 'survived']]\n",
    "X = X.dropna()\n",
    "y = X[['survived']]\n",
    "X = X[['age', 'fare']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = sklearn.tree.DecisionTreeClassifier().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(X.columns, mdl.feature_importances_)\n",
    "plt.gca().set_ylabel('Relative importance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `fare` feature is more informative than the `age` parameter.\n",
    "\n",
    "### Brute Force\n",
    "\n",
    "Another thing we can do is iterate over features to find the best combination. Let's use `mlxtend` to implement this for us (and borrow an example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlxtend > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mlxtend.data import wine_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X, y = wine_data()\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, \n",
    "                                                   stratify=y,\n",
    "                                                   test_size=0.3,\n",
    "                                                   random_state=1)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "sfs1 = SFS(estimator=knn, \n",
    "           k_features=(3, 10),\n",
    "           forward=True, \n",
    "           floating=False, \n",
    "           scoring='accuracy',\n",
    "           cv=5)\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), sfs1)\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "print('best combination (ACC: %.3f): %s\\n' % (sfs1.k_score_, sfs1.k_feature_idx_))\n",
    "# print('all subsets:\\n', sfs1.subsets_)\n",
    "plot_sfs(sfs1.get_metric_dict(), kind='std_err');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from mlxtend.evaluate import PredefinedHoldoutSplit\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Challenge\n",
    "\n",
    "Above is the infamous iris dataset.\n",
    "\n",
    "- Can you tell me which feature is the most informative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8: Cleaning Timeseries Data\n",
    "\n",
    "Your domain might involve timeseries data. Much of what we have discussed also applies to the time domain. However there is a significant amount of knowledge that has been accumulated under the specialisation of Signal Processing.\n",
    "\n",
    "But just to give you an idea, lets use the library `statsmodels` to give you some idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"../data/airline-passengers.csv\"\n",
    "airline = pd.read_csv(url)\n",
    "airline.columns = ['month', 'passengers']\n",
    "airline['month'] = pd.to_datetime(airline['month'])\n",
    "airline.set_index('month', inplace=True)\n",
    "airline.index = pd.DatetimeIndex(airline.index.values,freq=airline.index.inferred_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let me show you how to do some exponential smoothing. This is a simple averaging technique to perform \"noise reduction\". Note that we're not really reducing any noise, we're just saying that the rolling average of the data is a better representation of that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import SimpleExpSmoothing\n",
    "\n",
    "airline.plot()\n",
    "\n",
    "# Simple Exponential Smoothing\n",
    "for alpha in [0.3, 0.1]:\n",
    "    fit = SimpleExpSmoothing(airline).fit(smoothing_level=alpha,optimized=False)\n",
    "    fit.fittedvalues.rename(r'$\\alpha={}$'.format(alpha)).plot()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove a few samples to simulate some `NaN`s. Then we can perform some interpolation to fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_corrupt = airline.copy()\n",
    "airline_corrupt.iloc[10:11] = np.nan\n",
    "airline_corrupt.iloc[30:35] = np.nan\n",
    "airline_corrupt.iloc[60:100] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_corrupt.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_corrupt.interpolate().plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oneliner!\n",
    "\n",
    "Note how pandas in using a linear implementation of interpolation. So we lose a lot of the seasonality in the data. \n",
    "\n",
    "Also note that the bigger the gap, the worse it seems to perform. We can barely see the single `NaN` around position 10.\n",
    "\n",
    "Still very good for such little code though!\n",
    "\n",
    "### Bonus: Seasonal decomposition\n",
    "\n",
    "I thought I'd just add this as it's related. If you have seasonal data, `statsmodels` does a really nice job of decomposing the data to show you the periodicity and trend in the data. Take a look below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "result = seasonal_decompose(airline, model='multiplicative')\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_outliers = airline.copy()\n",
    "airline_outliers.iloc[10:11] = 0\n",
    "airline_outliers.iloc[30:35] = 10000\n",
    "airline_outliers.iloc[60:100] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Challenge\n",
    "\n",
    "The dataset above contains outliers.\n",
    "\n",
    "- Can you find the outliers?\n",
    "- Can you remove the outliers and replace with an interpolation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
